{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zriTdjauH8iQ"
      },
      "source": [
        "!pip install transformers\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQiRPWWHlSgv"
      },
      "source": [
        "### Using pre-trained transformers\n",
        "_for fun and profit_\n",
        "\n",
        "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
        "\n",
        "\n",
        "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
        "\n",
        "A typical pipeline includes:\n",
        "* pre-processing, e.g. tokenization, subword segmentation\n",
        "* a backbone model, e.g. bert finetuned for classification\n",
        "* output post-processing\n",
        "\n",
        "Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP1KFtvLlJHR"
      },
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "print(classifier(\"BERT is amazing!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYUNuyXMn5l9"
      },
      "source": [
        "import base64\n",
        "data = {\n",
        "    'arryn': 'As High as Honor.',\n",
        "    'baratheon': 'Ours is the fury.',\n",
        "    'stark': 'Winter is coming.',\n",
        "    'tyrell': 'Growing strong.'\n",
        "}\n",
        "\n",
        "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
        "<...>\n",
        "outputs = <YOUR CODE: dict (house name) : True if positive, False if negative>\n",
        "\n",
        "assert sum(outputs.values()) == 3 and outputs[base64.decodestring(b'YmFyYXRoZW9u\\n').decode()] == False\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRDhIH-XpSNo"
      },
      "source": [
        "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa-8noIllRbZ"
      },
      "source": [
        "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
        "MASK = mlm_model.tokenizer.mask_token\n",
        "\n",
        "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
        "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxeG1Y5pwX1"
      },
      "source": [
        "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
        "mlm_model(<YOUR PROMPT>)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJxRFzCSq903"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRux8Qp2hkXr"
      },
      "source": [
        "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
        " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
        " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
        " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
        "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
        "\"\"\"\n",
        "\n",
        "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
        "ner_model = <YOUR CODE>\n",
        "\n",
        "named_entities = ner_model(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf57MRzSiSON"
      },
      "source": [
        "print('OUTPUT:', named_entities)\n",
        "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
        "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
        "print(\"All tests passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULMownz6sP9n"
      },
      "source": [
        "### The building blocks of a pipeline\n",
        "\n",
        "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
        "* `Tokenizer` - converts from strings to token ids and back\n",
        "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
        "\n",
        "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMJbV0QVsO0Q"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgSPHKPRxG6U"
      },
      "source": [
        "lines = [\n",
        "    \"Luke, I am your father.\",\n",
        "    \"Life is what happens when you're busy making other plans.\",\n",
        "    ]\n",
        "\n",
        "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
        "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "for key in tokens_info:\n",
        "    print(key, tokens_info[key])\n",
        "\n",
        "print(\"Detokenized:\")\n",
        "for i in range(2):\n",
        "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJkbHxERyfL4"
      },
      "source": [
        "# You can now apply the model to get embeddings\n",
        "with torch.no_grad():\n",
        "    token_embeddings, sentence_embedding = model(**tokens_info)\n",
        "\n",
        "print(sentence_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMpzCoIL0Mmw"
      },
      "source": [
        "### The search for similar questions.\n",
        "\n",
        "Remeber week01 where you used GloVe embeddings to find related questions? That was.. cute, but far from state of the art. It's time to **really** solve this task using context-aware embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziLqtAcD3CXX"
      },
      "source": [
        "# download the data:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmKr7Eza3PZ6"
      },
      "source": [
        "__Main task(3 pts):__ \n",
        "* Implement a function that takes a text string and finds top-k most similar questions from `quora.txt`\n",
        "* Demonstrate your function using at least 5 examples\n",
        "\n",
        "There are no prompts this time: you will have to write everything from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsr4Jr0gzPvf"
      },
      "source": [
        "<A whole lot of your code. Feel free to format it as you see fit>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHEC6o7uAfgQ"
      },
      "source": [
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "__Bonus demo:__ transformer language models. \n",
        "\n",
        "`/* No points awarded for this task, but its really cool, we promise :) */`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWCajBGcAern"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
        "\n",
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "num_steps = 1024\n",
        "line_length, max_length = 0, 70\n",
        "\n",
        "print(end=tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(num_steps):\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens], device=device))[0]\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "\n",
        "    next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
        "    # YOUR TASK: change the code so that it performs nucleus sampling\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length >= max_length:\n",
        "        line_length = 0\n",
        "        print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vij7Gc1wOaq"
      },
      "source": [
        "Transformers knowledge hub: https://huggingface.co/transformers/"
      ]
    }
  ]
}
